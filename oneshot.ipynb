{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the one-shot learning model\n",
    "# Adapted from https://github.com/hlamba28/One-Shot-Learning-with-Siamese-Networks to suit my 224x224x3 data\n",
    "# Modifications include redesigning the model architecture\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization, MaxPooling2D, Concatenate\n",
    "from tensorflow.keras.layers import Lambda, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy.random as rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 174, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "data = np.load('../df6.npz', allow_pickle=True)\n",
    "data = dict(data)\n",
    "\n",
    "Xtrain = data['x_train']\n",
    "train_classes = data['y_train']\n",
    "Xv = data['x_test']\n",
    "val_classes = data['y_test']\n",
    "Xtrain = preprocess_input(Xtrain)\n",
    "Xval = preprocess_input(Xv)\n",
    "print(Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte = data['x_test']\n",
    "test_classes = data['y_test']\n",
    "Xtest = preprocess_input(Xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 20, 224, 224, 3) (100,)\n"
     ]
    }
   ],
   "source": [
    "ux = []\n",
    "uy = []\n",
    "for i in range(5):\n",
    "    uy.extend([i] * 20)\n",
    "    \n",
    "tpath = '../unseen'\n",
    "for dir in os.listdir(tpath):\n",
    "    path = os.path.join(tpath, dir)\n",
    "    if not os.path.isdir(path): continue\n",
    "    fns = os.listdir(path)\n",
    "    imgs = []\n",
    "    for fn in fns:\n",
    "        img = cv2.imread(os.path.join(path, fn))\n",
    "        if img is None: continue\n",
    "        imgs.append(img)\n",
    "    ux.append(np.array(imgs))\n",
    "\n",
    "ux = np.array(ux)\n",
    "uy = np.array(uy)\n",
    "print(ux.shape, uy.shape)\n",
    "uxp = preprocess_input(ux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.models.load_model('../model/dense2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, dtype, name=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bias(shape, dtype, name=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, regularizers, optimizers\n",
    "from tensorflow.keras.applications.densenet import DenseNet121 #resnet_v2 import ResNet50V2\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "def get_siamese_model(input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu',\n",
    "                     kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(DenseNet121(weights='imagenet',  \n",
    "                 include_top=False,\n",
    "                 pooling='avg'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Activation('sigmoid'))\n",
    "    '''\n",
    "    \n",
    "    '''res = np.load('../res.npz',allow_pickle = True)\n",
    "    res = dict(res)['wb']\n",
    "    model.layers[0].set_weights(m.layers[1].get_weights())\n",
    "    model.layers[0].trainable = False'''\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    L1_distance = BatchNormalization(axis = 1)(L1_distance)\n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1,activation='sigmoid', bias_initializer=initialize_bias)(L1_distance)\n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 4096)         420642112   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 4096)         0           sequential[0][0]                 \n",
      "                                                                 sequential[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 4096)         16384       lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            4097        batch_normalization[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 420,662,593\n",
      "Trainable params: 420,654,401\n",
      "Non-trainable params: 8,192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model((224,224,3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr = 1e-4)\n",
    "#model.layers[2].trainable = False\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size,s=\"train\"):\n",
    "    \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "    if s == 'train':\n",
    "        X = Xtrain\n",
    "        categories = train_classes\n",
    "    else:\n",
    "        X = Xval\n",
    "        categories = val_classes\n",
    "    n_classes, n_examples, w, h, c = X.shape\n",
    "\n",
    "    # randomly sample several classes to use in the batch\n",
    "    categories = rng.choice(n_classes,size=(batch_size,),replace=False)\n",
    "    \n",
    "    # initialize 2 empty arrays for the input image batch\n",
    "    pairs=[np.zeros((batch_size, h, w, 3)) for i in range(2)]\n",
    "    \n",
    "    # initialize vector for the targets\n",
    "    targets=np.zeros((batch_size,))\n",
    "    \n",
    "    # make one half of it '1's, so 2nd half of batch has same class\n",
    "    targets[batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = rng.randint(0, n_examples)\n",
    "        pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 3)\n",
    "        idx_2 = rng.randint(0, n_examples)\n",
    "\n",
    "        # pick images of same class for 1st half, different for 2nd\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category  \n",
    "        else: \n",
    "            # add a random number to the category modulo n classes to ensure 2nd image has a different category\n",
    "            category_2 = (category + rng.randint(1,n_classes)) % n_classes\n",
    "        \n",
    "        pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h, 3)\n",
    "    \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, s=\"train\"):\n",
    "    \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "    while True:\n",
    "        pairs, targets = get_batch(batch_size,s)\n",
    "        yield (pairs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oneshot_task(N, s=\"val\", language=None):\n",
    "    \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
    "    if s == 'train':\n",
    "        X = Xtrain\n",
    "        categories = train_classes\n",
    "    else:\n",
    "        X = Xval\n",
    "        categories = val_classes\n",
    "    n_classes, n_examples, w, h, c = X.shape\n",
    "    \n",
    "    indices = rng.randint(0, n_examples,size=(N,))\n",
    "    if language is not None: # if language is specified, select characters for that language\n",
    "        low, high = categories[language]\n",
    "        if N > high - low:\n",
    "            raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "        categories = rng.choice(range(low,high),size=(N,),replace=False)\n",
    "\n",
    "    else: # if no language specified just pick a bunch of random letters\n",
    "        categories = rng.choice(range(n_classes),size=(N,),replace=False)            \n",
    "    true_category = categories[0]\n",
    "    ex1, ex2 = rng.choice(n_examples,replace=False,size=(2,))\n",
    "    test_image = np.asarray([X[true_category,ex1,:,:,:]]*N).reshape(N, w, h,3)\n",
    "    support_set = X[categories,indices,:,:,:]\n",
    "    support_set[0,:,:,:] = X[true_category,ex2]\n",
    "    support_set = support_set.reshape(N, w, h,3)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    #targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "    pairs = [test_image,support_set]\n",
    "    '''\n",
    "    out = ux[categories,indices,:,:,:]\n",
    "    out[0] = ux[true_category,ex2]\n",
    "    out = out.reshape(N, w, h,3)\n",
    "    outt = ux[true_category, ex1].reshape(w, h, 3)\n",
    "    for i in range(N):\n",
    "        cv2.imwrite(str(i)+'.jpg', out[i])\n",
    "    cv2.imwrite('t.jpg', outt)\n",
    "    print(indices, ex1, ex2)\n",
    "    '''\n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oneshot(model, N, k, s = \"val\", verbose = 0):\n",
    "    \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
    "    for i in range(k):\n",
    "        inputs, targets = make_oneshot_task(N,s)\n",
    "        probs = model.predict(inputs)\n",
    "        print(probs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct+=1\n",
    "    percent_correct = (100.0 * n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "evaluate_every = 200 # interval for evaluating on one-shot tasks\n",
    "batch_size = 23\n",
    "n_iter = 1000 # No. of training iterations\n",
    "N_way = 5 # how many classes for testing one-shot tasks\n",
    "n_val = 30 # how many one-shot tasks to validate on\n",
    "best = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './weights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process!\n",
      "-------------------------------------\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 200 iterations: 0.5915686845779419 mins\n",
      "Train Loss: 0.6315654516220093\n",
      "Evaluating model on 30 random 5 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.0% 5 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 30 random 5 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 93.33333333333333% 5 way one-shot learning accuracy \n",
      "\n",
      "Current best: 50.0, previous best: 40.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 400 iterations: 1.273361361026764 mins\n",
      "Train Loss: 0.437203973531723\n",
      "Evaluating model on 30 random 5 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.0% 5 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 30 random 5 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.66666666666667% 5 way one-shot learning accuracy \n",
      "\n",
      "Current best: 50.0, previous best: 50.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 600 iterations: 1.9575650175412496 mins\n",
      "Train Loss: 0.47477394342422485\n",
      "Evaluating model on 30 random 5 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.666666666666664% 5 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 30 random 5 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 93.33333333333333% 5 way one-shot learning accuracy \n",
      "\n",
      "Current best: 56.666666666666664, previous best: 50.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 800 iterations: 2.6898478627204896 mins\n",
      "Train Loss: 0.487286239862442\n",
      "Evaluating model on 30 random 5 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 83.33333333333333% 5 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 30 random 5 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 100.0% 5 way one-shot learning accuracy \n",
      "\n",
      "Current best: 83.33333333333333, previous best: 56.666666666666664\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1000 iterations: 3.500078539053599 mins\n",
      "Train Loss: 0.32337135076522827\n",
      "Evaluating model on 30 random 5 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.333333333333336% 5 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 30 random 5 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 100.0% 5 way one-shot learning accuracy \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "tf.keras.backend.clear_session()\n",
    "t_start = time.time()\n",
    "for i in range(1, n_iter+1):\n",
    "    (inputs,targets) = get_batch(batch_size)\n",
    "    loss = model.train_on_batch(inputs, targets)\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
    "        print(\"Train Loss: {0}\".format(loss)) \n",
    "        val_acc = test_oneshot(model, N_way, n_val, verbose=True)\n",
    "        train_acc = test_oneshot(model, N_way, n_val, s='train',verbose=True)\n",
    "        model.save_weights(os.path.join(model_path, 'weights.{}.h5'.format(i)))\n",
    "        if val_acc >= best:\n",
    "            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "            best = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 1 random 5 way one-shot learning tasks ... \n",
      "\n",
      "[ 2  3 12  3  7] 1 0\n",
      "[[0.9679754 ]\n",
      " [0.99317485]\n",
      " [0.8225033 ]\n",
      " [0.93332195]\n",
      " [0.8523745 ]]\n",
      "Got an average of 0.0% 5 way one-shot learning accuracy \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_oneshot(model, 5, 1, s = \"val\", verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(model_path, \"weights.1000.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
